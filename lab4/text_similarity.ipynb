{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity Metrics\n",
    "\n",
    "Exercise notebook\n",
    "\n",
    "Course: Algorytmy Tekstowe at AGH University"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and vectorization\n",
    "\n",
    "1. Preprocessing: Convert the text documents to lowercase and remove all punctuation marks (using regular expressions, for example).\n",
    "2. Vocabulary creation: Create a vocabulary by taking all unique words from all text documents.\n",
    "3. Word frequency vectors: Create two vectors, each representing the frequency of each word in the vocabulary in each text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    # Your code here:\n",
    "    # Convert the text to lowercase.\n",
    "    # Remove all punctuation marks;\n",
    "    text = text.lower()\n",
    "    return re.sub('[^a-z0-9]+', ' ', text)\n",
    "\n",
    "\n",
    "def text_to_vec(docs: list[str]) -> list[list[int]]:\n",
    "    # Your code here:\n",
    "    # Convert documents to numerical vectors.\n",
    "    # Preprocess documents with the preprocess() function.\n",
    "    # Represent documents as vectors of word frequencies, \n",
    "    # you will need to extract a vocabulary from all the documents.\n",
    "    freq_vecs = []\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_docs.append(preprocess(doc))\n",
    "    dictonary = defaultdict(lambda : 0)\n",
    "\n",
    "    for doc in new_docs:\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            dictonary[word] += 1\n",
    "\n",
    "    dict_by_id = defaultdict(lambda : 0)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for key, value in dictonary.items():\n",
    "        dict_by_id[key] = i\n",
    "        i += 1\n",
    "\n",
    "    for doc in new_docs:\n",
    "        vec = [0 for _ in range(len(dictonary))]\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            vec[dict_by_id[word]] += 1\n",
    "        freq_vecs.append(vec)\n",
    "    return freq_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "text_a = \"The quick brown fox jumped over the lazy dog.\"\n",
    "text_b = \"The lazy dog was jumped over by the quick brown fox.\"\n",
    "vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "\n",
    "\n",
    "assert(set(vec_a) == set([1, 1, 1, 2, 1, 1, 1, 1, 0, 0]))\n",
    "assert(set(vec_b) == set([1, 1, 1, 2, 1, 1, 1, 1, 1, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}= \\frac{\\sum\\limits_{i=1}^{n} A_i B_i}{\\sqrt{\\sum\\limits_{i=1}^{n} A_i^2} \\sqrt{\\sum\\limits_{i=1}^{n} B_i^2}}\n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &\\mathbf{A}\\text{ and }\\mathbf{B} \\text{ are the two vectors being compared}\\\\\n",
    "    &n \\text{ is the dimensionality of the vectors}\\\\\n",
    "    &\\theta \\text{ represents the angle between two vectors } \\mathbf{A} \\text{ and } \\mathbf{B} \\text{ in a high-dimensional space}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The dot product of $\\mathbf{A}$ and $\\mathbf{B}$ is divided by the product of their Euclidean lengths to normalize the result to a range of [-1, 1]. A value of 1 indicates that the two vectors are identical, while a value of -1 indicates that they are completely dissimilar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(text_a: str, text_b: str) -> float:\n",
    "    # Your code here:\n",
    "    # Implement the cosine similarity\n",
    "    def get_dot_product(vec1, vec2):\n",
    "        sum = 0\n",
    "        for i in range(len(vec1)):\n",
    "            sum += vec1[i] * vec2[i]\n",
    "        return sum\n",
    "\n",
    "    def power(x):\n",
    "        return x*x\n",
    "\n",
    "    def get_length(vec):\n",
    "        return math.sqrt(sum(map(power, vec)))\n",
    "\n",
    "\n",
    "    vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "    \n",
    "\n",
    "    cosine_similarity = get_dot_product(vec_a, vec_b) / (get_length(vec_a) * get_length(vec_b))\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "dist = cosine_similarity(text_a, text_b)\n",
    "assert(abs(dist - 0.91986) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice coefficient / SÃ¸rensen-Dice Index\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\text{Dice}(A, B) = \\frac{2 |A \\cap B|}{|A| + |B|} \n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &A \\text{ and } B \\text{ represent the two sets being compared} \\\\\n",
    "    &|A| \\text{ and } |B| \\text{ represent the cardinality (number of elements) of the sets} \\\\\n",
    "    &\\text{and } |A \\cap B| \\text{ represents the size of the intersection of the two sets}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dice(text_a: str, text_b: str) -> float:\n",
    "    # Your code here:\n",
    "    # Implement the Dice coefficient\n",
    "    dice = 0\n",
    "    docs = [text_a, text_b]\n",
    "    new_docs = []\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        new_docs.append(preprocess(doc))\n",
    "\n",
    "    for doc in new_docs:\n",
    "        words.append(set(doc.split()))\n",
    "    \n",
    "    \n",
    "\n",
    "    return 2 * len(words[0] & words[1]) / (len(words[0]) + len(words[1]))\n",
    "\n",
    "dice(text_a, text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "dist = dice(text_a, text_b)\n",
    "assert(abs(dist - 0.88888) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &d(x,y) \\text{ is the Euclidean distance} \\\\\n",
    "    &x_i, y_i \\text{ are the values of the i-th dimension of vectors } x \\text{ and } y \\\\\n",
    "    &n \\text{ is the number of dimensions in the vectors}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(text_a: str, text_b: str) -> float:\n",
    "    # Your code here:\n",
    "    # Implement the Euclidean distance\n",
    "    dist = 0\n",
    "    vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "    for i in range(len(vec_a)):\n",
    "        dist += (vec_a[i] - vec_b[i])**2\n",
    "    \n",
    "    return math.sqrt(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "\n",
    "dist = euclidean_distance(text_a, text_b)\n",
    "assert(abs(dist - 1.4142135) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCS - Longest Common Subsequence\n",
    "\n",
    "Longest, common, continuous subsequence of two sequences, aka \"the longest substring\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "\n",
    "def lcs(seq_a: Sequence[Any], seq_b: Sequence[Any]) -> int:\n",
    "    # Your code here:\n",
    "    # Implement the longest common subsequence calculation.\n",
    "    # It should work on any sequences, not only on strings.\n",
    "    len1 = len(seq_a) + 1  # wiersze\n",
    "    len2 = len(seq_b) + 1  # kolumny\n",
    "    tab = [[0 for _ in range(len2)] for _ in range(len1)]\n",
    "\n",
    "    for i in range(len1):\n",
    "        tab[i][0] = i\n",
    "    for i in range(len2):\n",
    "        tab[0][i] = i\n",
    "\n",
    "    for i in range(1, len1):\n",
    "        for j in range(1, len2):\n",
    "            if seq_a[i-1] != seq_b[j-1]:\n",
    "                tab[i][j] = min(tab[i-1][j-1] + 2, tab[i]\n",
    "                                [j-1] + 1, tab[i-1][j] + 1)\n",
    "            else:\n",
    "                tab[i][j] = min(tab[i-1][j-1], tab[i]\n",
    "                                [j-1] + 1, tab[i-1][j] + 1)\n",
    "\n",
    "\n",
    "    return int((len(seq_a) + len(seq_b) - tab[-1][-1])/2)\n",
    "\n",
    "def word_lcs(text_a: str, text_b: str) -> int:\n",
    "    # You code here:\n",
    "    # Using the above function implement the LCS algorithm for texts.\n",
    "    # Make sure it works on whole words, not on characters.\n",
    "    seq_a = preprocess(text_a).split()\n",
    "    seq_b = preprocess(text_b).split()\n",
    "\n",
    "    return lcs(seq_a, seq_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert lcs(\"banana\", \"ananas\") == 5\n",
    "assert word_lcs(text_a, text_b) == 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance\n",
    "\n",
    "The minimal number of operations that needs to be performed in order to turn sequence A into sequence B.\n",
    "\n",
    "Available operations:\n",
    "\n",
    "* Replace element\n",
    "* Remove element\n",
    "* Add element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def levenshtein(seq_a: Sequence[Any], seq_b: Sequence[Any]) -> int:\n",
    "    # Your code here:\n",
    "    # Implement the Levenshtein distance calculation.\n",
    "    # It should work on any sequences, not only on strings.\n",
    "\n",
    "    dist = 0\n",
    "    len1 = len(seq_a) + 1  # wiersze\n",
    "    len2 = len(seq_b) + 1  # kolumny\n",
    "    tab = [[0 for _ in range(len2)] for _ in range(len1)]\n",
    "\n",
    "    for i in range(len1):\n",
    "        tab[i][0] = i\n",
    "    for i in range(len2):\n",
    "        tab[0][i] = i\n",
    "\n",
    "    for i in range(1, len1):\n",
    "        for j in range(1, len2):\n",
    "            if seq_a[i-1] != seq_b[j-1]:\n",
    "                tab[i][j] = min(tab[i-1][j-1] + 1, tab[i]\n",
    "                                [j-1] + 1, tab[i-1][j] + 1)\n",
    "            else:\n",
    "                tab[i][j] = min(tab[i-1][j-1], tab[i]\n",
    "                            [j-1] + 1, tab[i-1][j] + 1)\n",
    "\n",
    "    # for i in range(len(tab)):\n",
    "    #     print(tab[i])\n",
    "    # print(\"WYNIK\")\n",
    "    return tab[-1][-1]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def word_levenshtein(text_a: str, text_b: str) -> int:\n",
    "    # You code here:\n",
    "    # Using the above function implement the LCS algorithm for texts.\n",
    "    # Make sure it works on whole words, not on characters.\n",
    "    seq_a = preprocess(text_a).split()\n",
    "    seq_b = preprocess(text_b).split()\n",
    "    \n",
    "    return levenshtein(seq_a, seq_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert levenshtein(\"banana\", \"ananas\") == 2\n",
    "assert word_levenshtein(text_a, text_b) == 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
