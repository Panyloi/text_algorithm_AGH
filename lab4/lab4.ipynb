{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import math\n",
    "from typing import Any, Sequence, Dict\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> str:\n",
    "    # Your code here:\n",
    "    # Convert the text to lowercase.\n",
    "    # Remove all punctuation marks;\n",
    "    text = text.lower()\n",
    "    return re.sub('[^a-z0-9]+', ' ', text)\n",
    "\n",
    "\n",
    "def text_to_vec(docs: list[str]) -> list[list[int]]:\n",
    "    # Your code here:\n",
    "    # Convert documents to numerical vectors.\n",
    "    # Preprocess documents with the preprocess() function.\n",
    "    # Represent documents as vectors of word frequencies, \n",
    "    # you will need to extract a vocabulary from all the documents.\n",
    "    freq_vecs = []\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_docs.append(preprocess(doc))\n",
    "    dictonary = defaultdict(lambda : 0)\n",
    "\n",
    "    for doc in new_docs:\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            dictonary[word] += 1\n",
    "\n",
    "    dict_by_id = defaultdict(lambda : 0)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for key, value in dictonary.items():\n",
    "        dict_by_id[key] = i\n",
    "        i += 1\n",
    "\n",
    "    for doc in new_docs:\n",
    "        vec = [0 for _ in range(len(dictonary))]\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            vec[dict_by_id[word]] += 1\n",
    "        freq_vecs.append(vec)\n",
    "    return freq_vecs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(text_a: str, text_b: str) -> float:\n",
    "    # Your code here:\n",
    "    # Implement the Euclidean distance\n",
    "    dist = 0\n",
    "    vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "    for i in range(len(vec_a)):\n",
    "        dist += (vec_a[i] - vec_b[i])**2\n",
    "    \n",
    "    return math.sqrt(dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(seq_a: Sequence[Any], seq_b: Sequence[Any]) -> int:\n",
    "    # Your code here:\n",
    "    # Implement the longest common subsequence calculation.\n",
    "    # It should work on any sequences, not only on strings.\n",
    "    len1 = len(seq_a) + 1  # wiersze\n",
    "    len2 = len(seq_b) + 1  # kolumny\n",
    "    tab = [[0 for _ in range(len2)] for _ in range(len1)]\n",
    "\n",
    "    for i in range(len1):\n",
    "        tab[i][0] = i\n",
    "    for i in range(len2):\n",
    "        tab[0][i] = i\n",
    "\n",
    "    for i in range(1, len1):\n",
    "        for j in range(1, len2):\n",
    "            if seq_a[i-1] != seq_b[j-1]:\n",
    "                tab[i][j] = min(tab[i-1][j-1] + 2, tab[i]\n",
    "                                [j-1] + 1, tab[i-1][j] + 1)\n",
    "            else:\n",
    "                tab[i][j] = min(tab[i-1][j-1], tab[i]\n",
    "                                [j-1] + 1, tab[i-1][j] + 1)\n",
    "\n",
    "\n",
    "    return int((len(seq_a) + len(seq_b) - tab[-1][-1])/2)\n",
    "\n",
    "def word_lcs(text_a: str, text_b: str) -> int:\n",
    "    # You code here:\n",
    "    # Using the above function implement the LCS algorithm for texts.\n",
    "    # Make sure it works on whole words, not on characters.\n",
    "    seq_a = preprocess(text_a).split()\n",
    "    seq_b = preprocess(text_b).split()\n",
    "\n",
    "    return lcs(seq_a, seq_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(text_a: str, text_b: str) -> float:\n",
    "    # Your code here:\n",
    "    # Implement the Dice coefficient\n",
    "    dice = 0\n",
    "    docs = [text_a, text_b]\n",
    "    new_docs = []\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        new_docs.append(preprocess(doc))\n",
    "\n",
    "    for doc in new_docs:\n",
    "        words.append(set(doc.split()))\n",
    "    \n",
    "    \n",
    "\n",
    "    return 2 * len(words[0] & words[1]) / (len(words[0]) + len(words[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(seq_a: Sequence[Any], seq_b: Sequence[Any]) -> int:\n",
    "    # Your code here:\n",
    "    # Implement the Levenshtein distance calculation.\n",
    "    # It should work on any sequences, not only on strings.\n",
    "\n",
    "    dist = 0\n",
    "    len1 = len(seq_a) + 1  # wiersze\n",
    "    len2 = len(seq_b) + 1  # kolumny\n",
    "    tab = [[0 for _ in range(len2)] for _ in range(len1)]\n",
    "\n",
    "    for i in range(len1):\n",
    "        tab[i][0] = i\n",
    "    for i in range(len2):\n",
    "        tab[0][i] = i\n",
    "\n",
    "    for i in range(1, len1):\n",
    "        for j in range(1, len2):\n",
    "            if seq_a[i-1] != seq_b[j-1]:\n",
    "                tab[i][j] = min(tab[i-1][j-1] + 1, tab[i]\n",
    "                                [j-1] + 1, tab[i-1][j] + 1)\n",
    "            else:\n",
    "                tab[i][j] = min(tab[i-1][j-1], tab[i]\n",
    "                            [j-1] + 1, tab[i-1][j] + 1)\n",
    "\n",
    "    # for i in range(len(tab)):\n",
    "    #     print(tab[i])\n",
    "    # print(\"WYNIK\")\n",
    "    return tab[-1][-1]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def word_levenshtein(text_a: str, text_b: str) -> int:\n",
    "    # You code here:\n",
    "    # Using the above function implement the LCS algorithm for texts.\n",
    "    # Make sure it works on whole words, not on characters.\n",
    "    seq_a = preprocess(text_a).split()\n",
    "    seq_b = preprocess(text_b).split()\n",
    "    \n",
    "    return levenshtein(seq_a, seq_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Your code here:\n",
    "    # Implement the cosine similarity\n",
    "    def get_dot_product(vec1, vec2):\n",
    "        sum = 0\n",
    "        for i in range(len(vec1)):\n",
    "            sum += vec1[i] * vec2[i]\n",
    "        return sum\n",
    "\n",
    "    def power(x):\n",
    "        return x*x\n",
    "\n",
    "    def get_length(vec):\n",
    "        return math.sqrt(sum(map(power, vec)))\n",
    "\n",
    "\n",
    "    vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "    \n",
    "\n",
    "    cosine_similarity = get_dot_product(vec_a, vec_b) / (get_length(vec_a) * get_length(vec_b))\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own testing of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(text, n=2):\n",
    "    ngram = {}\n",
    "    for i in range(len(text) - n + 1):\n",
    "        if text[i:i + n] in ngram:\n",
    "            ngram[text[i:i + n]] += 1\n",
    "        else:\n",
    "            ngram[text[i:i + n]] = 1\n",
    "    return ngram\n",
    "\n",
    "def ngram_length(ngram):\n",
    "    result = 0\n",
    "    for key, value in ngram.items():\n",
    "        result += value ** 2\n",
    "    return np.sqrt(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos metric\n",
    "def cosine_metric(x, y, n=3):\n",
    "    x_ngrams = ngrams(x, n)\n",
    "    y_ngrams = ngrams(y, n)\n",
    "    result = 0\n",
    "    for key, value in x_ngrams.items():\n",
    "        if key in y_ngrams:\n",
    "            result += value * y_ngrams[key]\n",
    "    if ngram_length(x_ngrams) * ngram_length(y_ngrams) == 0:\n",
    "        return 1\n",
    "    return 1 - result / (ngram_length(x_ngrams) * ngram_length(y_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = \"The quick brown fox jumped over the lazy dog.\"\n",
    "text_b = \"The lazy dog was jumped over by the quick brown fox.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1524293136404088\n",
      "0.16026757798440006\n"
     ]
    }
   ],
   "source": [
    "print(cosine_metric(text_a, text_b))\n",
    "print((1 -  cosine_similarity(text_a, text_b) )* 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess given text by dividing and transforming into d-dimension \"vector\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_from_text(text: str, d: int = 3) -> Dict:\n",
    "    result = defaultdict(lambda: 0)\n",
    "    for i in range(len(text) - d + 1):\n",
    "        result[text[i:i+d]] += 1\n",
    "    return result\n",
    "\n",
    "def vec_norm(vect: defaultdict | Any) -> float:\n",
    "    return math.sqrt(sum(map(lambda x: x*x, vect.values())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(text_a: str, text_b: str, d: int = 3) -> float:\n",
    "    vec_a = vec_from_text(text_a, d)\n",
    "    vec_b = vec_from_text(text_b, d)\n",
    "    result = 0.0\n",
    "\n",
    "    for key, value in vec_a.items():\n",
    "        if key in vec_b:\n",
    "            result += value * vec_b[key]\n",
    "\n",
    "    if vec_norm(vec_a) * vec_norm(vec_b) == 0:\n",
    "        return 1\n",
    "        \n",
    "    return 1 - result / (vec_norm(vec_a) * vec_norm(vec_b))\n",
    "    \n",
    "    # return get_dot_product(vec_a, vec_b) / (vec_norm(vec_a) * vec_norm(vec_b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.708203932499369"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_norm(vect_from_text(text_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1524293136404088\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(text_a, text_b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
